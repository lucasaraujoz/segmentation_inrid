================================================================================
EXPLICAÇÃO DA IMPLEMENTAÇÃO WAVELET SKIP 1
================================================================================

1. ESTRUTURA DO ENCODER (EfficientNet-B4)
--------------------------------------------------------------------------------
Input image: torch.Size([1, 3, 512, 512])

Saídas do encoder (features):
  features[0]: torch.Size([1, 3, 512, 512]) <- ENTRADA (não é skip)
  features[1]: torch.Size([1, 48, 256, 256]) <- Skip 1
  features[2]: torch.Size([1, 32, 128, 128]) <- Skip 2
  features[3]: torch.Size([1, 56, 64, 64]) <- Skip 3
  features[4]: torch.Size([1, 160, 32, 32]) <- Skip 4
  features[5]: torch.Size([1, 448, 16, 16]) <- BOTTLENECK

Onde aplicamos wavelet: features[1] (primeiro skip REAL)
  - Resolução: 256x256 (H/2, W/2)
  - Canais: 48
  - Motivo: maior resolução espacial = mais detalhes preservados

2. DECOMPOSIÇÃO WAVELET (Haar DWT 2D)
--------------------------------------------------------------------------------
Input (1 canal):     (256, 256)
Após DWT nível 1:
  - LL (cA):  (128, 128)  <- Aproximação (baixa freq) - DESCARTAMOS
  - LH (cH):  (128, 128)  <- Horizontal edges (alta freq) - USAMOS
  - HL (cV):  (128, 128)  <- Vertical edges (alta freq) - USAMOS
  - HH (cD):  (128, 128)  <- Diagonal edges (alta freq) - USAMOS

Por que descartar LL?
  - LL é versão suavizada da imagem original
  - Skip já tem essa informação
  - Queremos apenas os DETALHES extras (LH, HL, HH)

3. EXEMPLO NUMÉRICO DO FLUXO
--------------------------------------------------------------------------------

Entrada: skip1 com shape [2, 48, 256, 256]
  (batch=2, canais=48, height=256, width=256)

Passo 1: Para cada batch e cada canal, aplicar DWT:
  - Total: 2 × 48 = 96 transformadas wavelet
  - Cada transformada gera: LL, LH, HL, HH de 128×128

Passo 2: Empilhar coeficientes:
  - LH_all: [2, 48, 128, 128]
  - HL_all: [2, 48, 128, 128]
  - HH_all: [2, 48, 128, 128]

Passo 3: Upsample bilinear 128×128 → 256×256:
  - LH_up: [2, 48, 256, 256]
  - HL_up: [2, 48, 256, 256]
  - HH_up: [2, 48, 256, 256]

Passo 4: Concatenar com skip original:
  - skip1:  [2, 48, 256, 256]
  - LH_up:  [2, 48, 256, 256]
  - HL_up:  [2, 48, 256, 256]
  - HH_up:  [2, 48, 256, 256]
  concat → [2, 192, 256, 256]  (48 × 4 canais)

Passo 5: Reduzir canais com Conv 1×1:
  - Conv2d(192 → 48, kernel=1×1)
  - BatchNorm2d(48)
  - ReLU
  Saída: [2, 48, 256, 256]  (mesma shape que skip original!)

4. INTEGRAÇÃO NO UNET VIA HOOK
--------------------------------------------------------------------------------

Por que usar hook?
  - Decoder do SMP espera tupla de features na ordem certa
  - Não podemos modificar manualmente sem quebrar compatibilidade
  - Hook intercepta saída do encoder e modifica transparentemente

Como funciona:
  1. Modelo faz: output = encoder(x)
  2. Hook dispara ANTES de retornar output
  3. Hook modifica: output[1] = wavelet(output[1])
  4. Decoder recebe output modificado sem saber

Código do hook (models/unet_wavelet_skip1.py):

  def _custom_forward_hook(self, module, input, output):
      features = list(output)  # Converter tupla em lista
      features[1] = self.wavelet_skip1(features[1])  # Wavelet!
      return tuple(features)  # Devolver como tupla

Registro do hook (no __init__):
  self.base_model.encoder.register_forward_hook(self._custom_forward_hook)

5. IMPACTO NOS PARÂMETROS
--------------------------------------------------------------------------------
UNet normal:         20,225,834 parâmetros
UNet + wavelet:      20,235,146 parâmetros
Diferença:           9,312 parâmetros
Aumento:             0.05%

De onde vem a diferença:
  Conv 1×1: (192 → 48):
    Pesos: 192 × 48 × 1 × 1 = 9,216 parâmetros
  BatchNorm (48 canais):
    Pesos/bias: 48 × 2 = 96 parâmetros
  Total wavelet module: ~9,312 parâmetros

6. EXEMPLO VISUAL: O QUE WAVELET EXTRAI
--------------------------------------------------------------------------------

Imagine um pixel e seus vizinhos (simplificado):

  Original:          LL (aprox):        LH (horiz):
  100 120 100       110 110            -10 +10
  120 140 120       110 110            -10 +10
  100 120 100       110 110            -10 +10

  HL (vert):         HH (diag):
  -10 -10            0   0
  +10 +10            0   0
  -10 -10            0   0

Interpretação:
  - LL: suavização (média) - já temos no skip
  - LH: bordas horizontais (mudança esquerda→direita)
  - HL: bordas verticais (mudança cima→baixo)
  - HH: bordas diagonais (cantos)

Aplicação em lesões:
  - Exsudatos: pequenos, bordas bem definidas → HH alto
  - Hemorragias: formas irregulares → LH + HL destacam limites

7. VALIDAÇÃO RÁPIDA
--------------------------------------------------------------------------------

Testando forward pass...
Input:  torch.Size([2, 3, 512, 512])
Output: torch.Size([2, 2, 512, 512])
Status: ✓ OK

================================================================================
RESUMO EXECUTIVO
================================================================================

O que foi implementado:
  ✓ Wavelet Haar DWT 2D no primeiro skip (256×256, 48 canais)
  ✓ Extração de alta frequência: LH + HL + HH (bordas)
  ✓ Concatenação com skip original (não substitui)
  ✓ Redução de canais via Conv 1×1 + BN + ReLU
  ✓ Integração via hook (transparente para decoder)

Overhead:
  ✓ Parâmetros: +9,312 (+0.05%)
  ✓ Memória: ~20 MB extra no forward (batch=4)

Hipótese:
  Informação de alta frequência ajuda a detectar bordas finas
  de microlesões (exsudatos e hemorragias pequenas)

Baseline de comparação:
  - Sem CLAHE: 0.6501 Dice
  - Com wavelet: ???

Pronto para treinar!